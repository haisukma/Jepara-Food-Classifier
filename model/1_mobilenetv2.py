# -*- coding: utf-8 -*-
"""1. MobileNetV2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LvsfYYiIpntVnMDfeAObQbB01_5_1v_k
"""

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 30
LEARNING_RATE = 0.0001

base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
base_model.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)  # Dropout untuk mengurangi overfitting
output = Dense(3, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

import zipfile
with zipfile.ZipFile('/content/Dataset_Asli.zip', 'r') as zip_ref:
    zip_ref.extractall('./')

dataset_path = '/content/Dataset_Asli'

import os
import shutil
from sklearn.model_selection import train_test_split

# Folder tujuan untuk split data
train_path = os.path.join(dataset_path, 'train')  # Folder untuk train
test_path = os.path.join(dataset_path, 'test')    # Folder untuk test

# Buat folder untuk data train dan test jika belum ada
os.makedirs(train_path, exist_ok=True)
os.makedirs(test_path, exist_ok=True)

# Ambil semua folder kelas dalam dataset (misalnya adon_adon_coro, horok_horok, pindang_serani)
classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d)) and d not in ['train', 'test']]

# Proses split untuk setiap kelas
for class_name in classes:
    class_folder = os.path.join(dataset_path, class_name)
    image_files = []

    # Menelusuri subfolder untuk menemukan file gambar
    for root, dirs, files in os.walk(class_folder):
        for file in files:
            if file.lower().endswith(('png', 'jpg', 'jpeg')):  # Pastikan file gambar
                image_files.append(os.path.join(root, file))

    # Check if the class folder contains any images
    if not image_files:
        print(f"Warning: Class '{class_name}' has no images. Skipping split.")
        continue  # Skip to the next class

    # Split data dengan proporsi 70:30
    train_files, test_files = train_test_split(image_files, test_size=0.3, random_state=42)

    # Folder tujuan untuk masing-masing kelas di dalam train dan test
    class_train_folder = os.path.join(train_path, class_name)
    class_test_folder = os.path.join(test_path, class_name)

    # Buat folder untuk kelas di train dan test jika belum ada
    os.makedirs(class_train_folder, exist_ok=True)
    os.makedirs(class_test_folder, exist_ok=True)

    # Pindahkan file gambar ke folder masing-masing
    for file in train_files:
        shutil.move(file, os.path.join(class_train_folder, os.path.basename(file)))

    for file in test_files:
        shutil.move(file, os.path.join(class_test_folder, os.path.basename(file)))

    # Tampilkan jumlah gambar di train dan test folder untuk setiap kelas
    num_train_images = len(os.listdir(class_train_folder))
    num_test_images = len(os.listdir(class_test_folder))

    print(f"Class: {class_name}")
    print(f"  - Number of images in train: {num_train_images}")
    print(f"  - Number of images in test: {num_test_images}")

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2  # 80% training, 20% validation
)

train_generator = train_datagen.flow_from_directory(
    train_path,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

val_generator = train_datagen.flow_from_directory(
    train_path,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    verbose=1,
    restore_best_weights=True
)

history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=val_generator,
    callbacks=[early_stopping]
)

import matplotlib.pyplot as plt

# Plot akurasi
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
# plt.plot(history_finetune.history['accuracy'], label='Train Accuracy (Fine-tuning)')
# plt.plot(history_finetune.history['val_accuracy'], label='Validation Accuracy (Fine-tuning)')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training & Validation Accuracy')

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
# plt.plot(history_finetune.history['loss'], label='Train Loss (Fine-tuning)')
# plt.plot(history_finetune.history['val_loss'], label='Validation Loss (Fine-tuning)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training & Validation Loss')

plt.show()

base_model.trainable = True
for layer in base_model.layers[:-20]:  # Hanya fine-tune 20 layer terakhir
    layer.trainable = False

from tensorflow.keras.layers import Dropout, Dense
from tensorflow.keras.regularizers import l2

# Ambil layer terakhir sebelum output
x = base_model.output

x = GlobalAveragePooling2D()(x)

# Tambahkan Dropout
x = Dropout(0.3)(x)

# Tambahkan Dense layer dengan L2 Regularization
x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)

# Output layer
output = Dense(3, activation='softmax')(x)  # Sesuaikan jumlah kelas

# Buat model baru dengan Functional API
model = tf.keras.Model(inputs=base_model.input, outputs=output)

# Compile ulang model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    monitor='val_loss', patience=3, restore_best_weights=True
)

history_finetune = model.fit(
    train_generator,
    epochs=20,  # Tambahkan beberapa epoch, tapi bisa berhenti lebih awal
    validation_data=val_generator,
    callbacks=[early_stopping]
)

import matplotlib.pyplot as plt

# Plot akurasi
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history_finetune.history['accuracy'], label='Train Accuracy (Fine-tuning)')
plt.plot(history_finetune.history['val_accuracy'], label='Validation Accuracy (Fine-tuning)')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Fine-tuning: Training & Validation Accuracy')

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history_finetune.history['loss'], label='Train Loss (Fine-tuning)')
plt.plot(history_finetune.history['val_loss'], label='Validation Loss (Fine-tuning)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Fine-tuning: Training & Validation Loss')

plt.show()

test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    test_path,  # Ganti dengan path folder test
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False  # Jangan diacak agar prediksi bisa dibandingkan dengan label
)

test_loss, test_acc = model.evaluate(test_generator)
print(f"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

# Evaluasi pada data test
test_loss, test_acc = model.evaluate(test_generator)
print(f"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

# Ambil validation accuracy dari hasil training terakhir
val_acc = history_finetune.history['val_accuracy'][-1]  # Ambil nilai validation accuracy dari epoch terakhir

print(f"Validation Accuracy: {val_acc:.4f}")

# Bandingkan Test Accuracy & Validation Accuracy
if abs(test_acc - val_acc) <= 0.02:  # Toleransi selisih 2%
    print("✅ Model masih generalizable (tidak overfitting)")
elif test_acc < val_acc:
    print("⚠️ Model kemungkinan overfitting! Test accuracy lebih rendah dari validation accuracy.")
else:
    print("ℹ️ Model bekerja baik di test set, tetapi tetap perlu dicek lebih lanjut.")

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix

print("[INFO] Testing and evaluating...")

# Prediksi pada data uji
predictions = model.predict(test_generator)
y_pred = np.argmax(predictions, axis=1)

# Label sebenarnya
true_classes = test_generator.classes
class_labels = list(test_generator.class_indices.keys())

# Laporan klasifikasi
report = classification_report(true_classes, y_pred, target_names=class_labels)
print(report)

# Confusion Matrix
cm = confusion_matrix(true_classes, y_pred)
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

from google.colab import files
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Pastikan img_height dan img_width sudah didefinisikan
img_height = 224  # Sesuaikan dengan ukuran input model
img_width = 224

# Upload file gambar
uploaded = files.upload()

for fn in uploaded.keys():
    path = fn

    # Load gambar
    img = tf.keras.utils.load_img(path, target_size=(img_height, img_width))

    # Tampilkan gambar
    plt.imshow(img)
    plt.axis("off")
    plt.show()

    # Konversi gambar ke array
    x = tf.keras.utils.img_to_array(img)
    x = tf.expand_dims(x, axis=0)  # Tambahkan batch dimension

    # Preprocessing jika model menggunakan MobileNetV2
    x = tf.keras.applications.mobilenet_v2.preprocess_input(x)

    # Prediksi kelas
    predictions = model.predict(x)

    # Ambil index kelas dengan confidence tertinggi
    predicted_class_index = np.argmax(predictions[0])
    confidence_score = np.max(predictions[0])  # Ambil confidence tertinggi

    # Ambil label kelas
    class_labels = list(train_generator.class_indices.keys())  # Label kelas dari train_generator

    # Tampilkan hasil prediksi
    print(f"File: {fn}")
    print(f"Predicted Class: {class_labels[predicted_class_index]}")
    print(f"Confidence Score: {confidence_score:.2f}")

model.save("jepara_food_classifier.h5", save_format="h5")

pip install h5py

pip freeze > requirements.txt

